{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\wenjiananzhaung\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "614\n",
      "1581\n",
      "1954\n",
      "2459\n",
      "3780\n",
      "么有答案的样本6\n",
      "么有答案的样本0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import jieba.posseg as pseg\n",
    "from gensim import corpora\n",
    "from gensim.summarization import bm25\n",
    "\n",
    "#数据读取\n",
    "\n",
    "train = pd.read_csv(open(r\"C:\\Users\\qian_wang\\Desktop\\疫情\\NCPPolicies_train_20200301.csv\",encoding = \"utf-8\"),delimiter = \"\\t\")\n",
    "test = pd.read_csv(open(r\"C:\\Users\\qian_wang\\Desktop\\疫情\\NCPPolicies_test.csv\",encoding = \"utf-8\"),delimiter = \"\\t\")\n",
    "doc = pd.read_csv(open(r\"C:\\Users\\qian_wang\\Desktop\\疫情\\NCPPolicies_context_20200301.csv\",encoding = \"utf-8\"),delimiter = \"\\n\")\n",
    "doc_pro = pd.DataFrame()\n",
    "doc_pro[\"docid\"] = [text[0].split(\"\\t\")[0] for text in doc.values]\n",
    "doc_pro[\"text\"] = [text[0].split(\"\\t\")[1] for text in doc.values]\n",
    "#doc_pro去重\n",
    "#doc_pro.drop_duplicates(subset = [\"text\"],keep='first',inplace = True)\n",
    "train_doc = pd.merge(train,doc_pro,how = \"left\",on = \"docid\")\n",
    "\n",
    "#生成训练以及验证数据文件\n",
    "def data_json(data):\n",
    "    res = {}\n",
    "    count=0\n",
    "    data_dic = []\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        question = \" \".join(data.iloc[i,2])\n",
    "        text = \" \".join(\"\".join(data.iloc[i,4].split()))\n",
    "        answers = \" \".join(\"\".join(data.iloc[i,3].split()))\n",
    "        answers_start = text.find(answers)\n",
    "        if answers_start==-1:\n",
    "            count+=1\n",
    "            print(i)\n",
    "        dic = {}\n",
    "        dic[\"context\"] = text\n",
    "        \n",
    "        dic[\"qas\"] = [{\"answers\":[{\"answer_start\":answers_start,\"text\":answers}],\"question\":question,\"id\":data.iloc[i,0]}]\n",
    "        data_dic.append({\"title\":\"疫情\",\"paragraphs\":[dic]})\n",
    "                      \n",
    "    res[\"data\"] = data_dic\n",
    "    print(\"么有答案的样本{}\".format(count))\n",
    "    return res\n",
    "train_json_08 = data_json(train_doc.iloc[:4000,:])\n",
    "\n",
    "with open(r'C:\\Users\\qian_wang\\Desktop\\疫情\\train_0.8.json','w',encoding = 'utf-8') as f:\n",
    "    json.dump(train_json_08,f,ensure_ascii=False)\n",
    "f.close()\n",
    "valid_json_02 = data_json(train_doc.iloc[4000:5000,:])\n",
    "with open(r'C:\\Users\\qian_wang\\Desktop\\疫情\\valid_0.2.json','w',encoding = 'utf-8') as f:\n",
    "    json.dump(valid_json_02,f,ensure_ascii=False)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\QIAN_W~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.757 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#bm25训练\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "for i in open(r\"C:\\Users\\qian_wang\\Desktop\\疫情\\stop_words.txt\",encoding = \"utf-8\").readlines():\n",
    "    stop_words.append(i.strip())\n",
    "stop_flag = ['x', 'c', 'u','d', 'p', 't', 'uj', 'm', 'f', 'r']\n",
    "def tokenization(texts):\n",
    "    result = []\n",
    "    for word in texts[\"text\"].values:\n",
    "        words = pseg.cut(word)\n",
    "        out = []\n",
    "        for word, flag in words:\n",
    "            \n",
    "            if flag not in stop_flag and word not in stop_words:\n",
    "            #if flag not in stop_flag:\n",
    "                out.append(word)\n",
    "        result.append(out)\n",
    "    return result\n",
    "#构建语料库\n",
    "corpus = []\n",
    "corpus = tokenization(doc_pro)\n",
    "#构建bm25模型\n",
    "bm25Model = bm25.BM25(corpus)\n",
    "average_idf = sum(map(lambda k: float(bm25Model.idf[k]),bm25Model.idf.keys())) / len(bm25Model.idf.keys())\n",
    "#判断相关性\n",
    "def top_correlation_index(query,bm25Model,average_idf,stop_flag):\n",
    "\n",
    "    words = pseg.cut(query)\n",
    "    out = []\n",
    "    for word, flag in words:\n",
    "        if flag not in stop_flag:\n",
    "                out.append(word)\n",
    "            \n",
    "    scores = bm25Model.get_scores(out,average_idf)\n",
    "    \n",
    "    return scores.index(max(scores))\n",
    "\n",
    "#构建docid-index\n",
    "\n",
    "docid_index = {}\n",
    "for i,docid in enumerate(doc_pro[\"docid\"].values):\n",
    "    docid_index[i] = docid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "264\n",
      "382\n",
      "439\n",
      "561\n",
      "762\n",
      "868\n",
      "1084\n",
      "1090\n",
      "1193\n",
      "1202\n",
      "1227\n",
      "1310\n",
      "1350\n",
      "1439\n",
      "1542\n",
      "么有答案的样本16\n"
     ]
    }
   ],
   "source": [
    "#测试样本生成，这里取bm25top3样本连接预测\n",
    "def topN_doc(query,bm25Model,average_idf,stop_flag,N):\n",
    "    words = pseg.cut(query)\n",
    "    out = []\n",
    "    for word, flag in words:\n",
    "        if flag not in stop_flag:\n",
    "                out.append(word)\n",
    "            \n",
    "    scores = bm25Model.get_scores(out,average_idf)\n",
    "    res = {}\n",
    "    for i,v in enumerate(scores):\n",
    "        res[i] = v\n",
    "        \n",
    "    return sorted(res.items(),key = lambda x:x[1],reverse = True)[:N]\n",
    "def test_sample_docidN(test,docid_index,N):\n",
    "    id_ = []\n",
    "    question_ = []\n",
    "    docid_ = []\n",
    "    scores = []\n",
    "    for i in range(test.shape[0]):\n",
    "        dic = topN_doc(test[\"question\"][i],bm25Model,average_idf,stop_flag,N)\n",
    "        docids = []\n",
    "        score_dic = []\n",
    "        \n",
    "        #添加分数，分数归一化\n",
    "        sum_ = sum([i[1] for i in dic]) \n",
    "        for k,v in dic:\n",
    "            docids.append(docid_index[k])\n",
    "            score_dic.append(v/sum_) \n",
    "        id_.extend([test[\"id\"][i]]*N)\n",
    "        docid_.extend(docids)\n",
    "        question_.extend([test[\"question\"][i]]*N)\n",
    "        scores.extend(score_dic)\n",
    "    return pd.DataFrame({\"id\":id_,\"question\":question_,\"docid\":docid_,\"bm25_score\":scores})\n",
    "def test_N(test,N):\n",
    "    ids = []\n",
    "    docids = []\n",
    "    questions = []\n",
    "    texts = []\n",
    "    for i in range(0,test.shape[0],20):\n",
    "        id_ = test[\"id\"][i]\n",
    "        docid_ = test[\"docid\"][i]\n",
    "        question_ = test[\"question\"][i]\n",
    "        text_ = \"\"\n",
    "        text_temp = set(test[\"text\"][i:i+N])\n",
    "        for text in text_temp:\n",
    "            text_+=text\n",
    "        ids.append(id_)\n",
    "        docids.append(docid_)\n",
    "        questions.append(question_)\n",
    "        texts.append(text_)\n",
    "    return pd.DataFrame({\"id\":ids,\"docid\":docids,\"question\":questions,\"text\":texts})\n",
    "\n",
    "test_sample_N =  test_sample_docidN(test,docid_index,20)\n",
    "test_sample_N_docid = pd.merge(test_sample_N,doc_pro,how = \"left\",on = \"docid\")\n",
    "\n",
    "test_final = test_N(test_sample_N_docid,3)\n",
    "test_final[\"answer\"] = [\"1\"]*test_final.shape[0]\n",
    "test_doc = test_final[[\"id\",\"docid\",\"question\",\"answer\",\"text\"]]\n",
    "test_json = data_json(test_doc)\n",
    "with open(r'C:\\Users\\qian_wang\\Desktop\\疫情\\test_torch_0413_N3_json.json','w',encoding = 'utf-8') as f:\n",
    "    json.dump(test_json,f,ensure_ascii=False)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#结果集生成提交文件\n",
    "test_ = json.load(open(r\"C:\\Users\\qian_wang\\Desktop\\疫情\\predictions_0413.json\",encoding = \"utf-8\"))\n",
    "test_dic = {}\n",
    "for i in test_.keys():\n",
    "    test_dic[i] = \"\".join(test_[i].split())    \n",
    "test_doc[\"answer\"] = test_dic.values()\n",
    "test_doc[[\"id\",\"docid\",\"answer\"]].to_csv(r\"C:\\Users\\qian_wang\\Desktop\\疫情\\test_predictions_0413.csv\",encoding = \"utf-8\",sep = \"\\t\",index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
